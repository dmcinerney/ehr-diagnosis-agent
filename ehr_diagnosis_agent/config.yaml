data:
  path: /work/frink/mcinerney.de/datasets/mimic-iii/physionet.org/files/mimiciii/1.4/preprocessed
  dataset: reports_and_codes3
# write_to_cache_slice: null
write_to_cache_slice: '10000,12000'
training:
  # limit_train_examples: null
  limit_train_size: 10000
  resume_from: null
  # resume_from: /work/frink/mcinerney.de/ehr-diagnosis-agent/ehr_diagnosis_agent/wandb/run-20230604_010745-6xacw8w1/files/ckpt_epoch=8_updates=986.pt
  # resume_from: /work/frink/mcinerney.de/ehr-diagnosis-agent/ehr_diagnosis_agent/wandb/run-20230612_161749-gjcsiong/files/ckpt_epoch=3_updates=141.pt
  num_epochs: 100
  num_episodes: 4
  batch_size: 1
  accumulate_grad_batches: 16
  # actor_lr: 1e-6
  actor_lr: 1e-5
  # critic_lr: 3e-6
  critic_lr: 1e-5
  max_reports_considered: 48
  max_trajectory_length: null
  checkpoint_every: 1
  # objective_optimization: ppo_gae
  objective_optimization: supervised
  clear_gpu: true
  freeze_actor: null
  # freeze_actor: "everything_but_query"
ppo_gae:
  gamma: 0.99
  lam: 0.95
  epsilon: .5
  sub_epochs: 1
  use_adv_norm: true
  # entropy_coefficient: .01
  entropy_coefficient: 0
  use_grad_clip: true
  log_reward: false
  # log_reward: true
  skip_risk_prediction: false
  # skip_risk_prediction: true
  # only_train_critic: false
  only_train_critic: true
supervised:
  sub_epochs: 1
  use_grad_clip: true
  entropy_coefficient: 0
ppo_dae:
  epsilon: .5
env:
  model_name: 'google/flan-t5-xxl'
  reward_type: 'continuous_independent'
  num_future_diagnoses_threshold: 1
  cache_path: /work/frink/mcinerney.de/ehr-diagnosis-agent/ehr_diagnosis_agent/.cache/environment_cache_1
  top_k_evidence: 3
actor:
  # type: normal
  # type: dirichlet
  type: beta
  normal_params:
    model_name: emilyalsentzer/Bio_ClinicalBERT
    # model_name: roberta-base
    embedding_batch_size: 32
    stddev_min: .02
    stddev_max: 1
  dirichlet_params:
    model_name: emilyalsentzer/Bio_ClinicalBERT
    # model_name: roberta-base
    embedding_batch_size: 32
    # concentration_min: .2
    concentration_min: 1
    # concentration_min: 5
  beta_params:
    model_name: emilyalsentzer/Bio_ClinicalBERT
    # model_name: roberta-base
    embedding_batch_size: 32
    concentration_min: 1
    init_concentration0: 100
critic:
  model_name: emilyalsentzer/Bio_ClinicalBERT
  # model_name: roberta-base
  embedding_batch_size: 32
